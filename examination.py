# examination.py

import streamlit as st
import yfinance as yf
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose, STL
from datetime import datetime
import numpy as np
from statsmodels.tsa.stattools import adfuller, kpss
from scipy.stats import ks_2samp
from scipy import stats
import matplotlib.pyplot as plt
import warnings

def run(company_name, ticker_symbol, start_date, end_date):
    st.title("üîç Time Series Examination")

    df = yf.download(ticker_symbol, start=start_date, end=end_date)
    if df.empty:
        st.warning("No data available for the selected date range.")
        return

    df = df.reset_index()
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)
    ts = df['Close'].dropna()

    st.subheader("1Ô∏è‚É£ Time Series Overview")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        This section shows the **raw time series of closing stock prices** over the selected date range.

        - Helps you **visually inspect** the trend, volatility, and any visible seasonality.
        - Useful for getting a **baseline feel** for the structure of the data before applying any transformations or tests.
        """)
    st.line_chart(ts)

    st.subheader("2Ô∏è‚É£ Classical Decomposition")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        **Classical decomposition** splits the series into 3 components:
        - **Trend**: long-term movement
        - **Seasonal**: repeated cycles (e.g., weekly, monthly)
        - **Residual**: random noise

        **Model used** here is additive:  
        `Observed = Trend + Seasonal + Residual`

        - Useful to detect patterns and understand whether your series is **structured** or mostly **random**.
        """)
    freq = st.slider("Select frequency (days)", min_value=5, max_value=60, value=30)
    try:
        result = seasonal_decompose(ts, model='additive', period=freq)
        fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)
        result.observed.plot(ax=axes[0], title='Observed')
        result.trend.plot(ax=axes[1], title='Trend')
        result.seasonal.plot(ax=axes[2], title='Seasonal')
        result.resid.plot(ax=axes[3], title='Residual')
        st.pyplot(fig)
    except Exception as e:
        st.error(f"Decomposition failed: {e}")

    st.subheader("3Ô∏è‚É£ STL Decomposition (LOESS)")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        **STL (Seasonal-Trend decomposition using LOESS)** is a more flexible and robust method than classical decomposition.

        It splits the series into:
        - **Trend**
        - **Seasonal**
        - **Residual**

        **Why it's better:**
        - Handles **nonlinear trends**
        - More robust to **outliers**
        - Allows seasonal components to **change over time**

        Use it to get a clearer view of underlying structure in real-world, noisy data.
        """)
    try:
        stl = STL(ts, period=freq, robust=True)
        res = stl.fit()
        fig = res.plot()
        st.pyplot(fig)
    except Exception as e:
        st.error(f"STL Decomposition failed: {e}")

    st.subheader("4Ô∏è‚É£ Stationarity Transformations")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        Stationarity means the **statistical properties** (mean, variance) of the series do **not change over time**.

        These transformations help in:
        - **Removing trend or variance instability**
        - Making the series **stationary**, which is often a requirement for forecasting models

        **Common transformations:**
        - **Log / Square Root**: stabilizes variance
        - **Differencing**: removes trend
        - **Power / Box-Cox**: adjusts skewness and distribution

        Apply one or a combination to make your series more model-ready.
        """)
    transformations = st.multiselect(
        "Apply transformations", ["Log", "Square Root", "First Difference", "Power (2)", "Box-Cox"]
    )
    ts_transformed = ts.copy()

    for t in transformations:
        try:
            if t == "Log":
                ts_transformed = np.log(ts_transformed.replace(0, np.nan)).dropna()
            elif t == "Square Root":
                ts_transformed = np.sqrt(ts_transformed)
            elif t == "First Difference":
                ts_transformed = ts_transformed.diff().dropna()
            elif t == "Power (2)":
                ts_transformed = ts_transformed ** 2
            elif t == "Box-Cox":
                ts_positive = ts_transformed[ts_transformed > 0].dropna()

                try:
                    boxcox_values, _ = stats.boxcox(ts_positive.values.flatten())
                    ts_transformed = pd.Series(boxcox_values, index=ts_positive.index)
                except Exception as e:
                    st.warning(f"Box-Cox transformation failed: {e}")

        except Exception as e:
            st.warning(f"{t} transformation failed: {e}")

    st.line_chart(ts_transformed)

    st.subheader("5Ô∏è‚É£ ADF Test")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        The **Augmented Dickey-Fuller (ADF) Test** checks for **unit root**, which is a sign of **non-stationarity**.

        - **Null Hypothesis (H‚ÇÄ)**: Series has a unit root ‚Üí non-stationary
        - **Alternative (H‚ÇÅ)**: Series is stationary

        **How to interpret:**
        - p-value < 0.05 ‚Üí ‚úÖ likely **stationary**
        - p-value ‚â• 0.05 ‚Üí ‚ùå likely **non-stationary**

        Use ADF to confirm if further transformation is needed.
        """)
    adf = adfuller(ts_transformed)
    st.write(f"ADF Statistic: {adf[0]:.4f}")
    st.write(f"p-value: {adf[1]:.4f}")
    for k, v in adf[4].items():
        st.write(f"Critical Value {k}: {v:.4f}")
    # Interpretation
    if adf[1] < 0.05:
        st.success("‚úÖ ADF Test Conclusion: The series is likely **stationary** (p < 0.05).")
    else:
        st.warning("‚ùå ADF Test Conclusion: The series is likely **non-stationary** (p ‚â• 0.05).")

    st.subheader("6Ô∏è‚É£ KPSS Test")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        The **KPSS Test** is the **mirror opposite** of the ADF test.

        - **Null Hypothesis (H‚ÇÄ)**: Series is stationary
        - **Alternative (H‚ÇÅ)**: Series is non-stationary

        **How to interpret:**
        - p-value < 0.05 ‚Üí ‚ùå likely **non-stationary**
        - p-value ‚â• 0.05 ‚Üí ‚úÖ likely **stationary**

        Use it alongside ADF for stronger conclusions.
        """)
    try:
        kpss_stat, p_value, _, crit = kpss(ts_transformed, regression='c', nlags="auto")
        st.write(f"KPSS Statistic: {kpss_stat:.4f}")
        st.write(f"p-value: {p_value:.4f}")
        for k, v in crit.items():
            st.write(f"Critical Value {k}: {v}")
    except Exception as e:
        st.error(f"KPSS failed: {e}")
     # Interpretation
    if p_value < 0.05:
        st.warning("‚ùå KPSS Test Conclusion: The series is likely **non-stationary** (p < 0.05).")
    else:
        st.success("‚úÖ KPSS Test Conclusion: The series is likely **stationary** (p ‚â• 0.05).")

    st.markdown("### üìå Overall Stationarity Summary")
    if adf[1] < 0.05 and p_value >= 0.05:
        st.success("‚úÖ Both ADF and KPSS agree: The series is **stationary**.")
    elif adf[1] >= 0.05 and p_value < 0.05:
        st.warning("‚ö†Ô∏è Both ADF and KPSS agree: The series is **non-stationary**.")
    else:
        st.info("‚ÑπÔ∏è ADF and KPSS disagree. Consider further analysis or transformation.")


    st.subheader("7Ô∏è‚É£ Kolmogorov-Smirnov Test")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        The **KS Test** compares two samples to check if they come from the **same distribution**.

        Here, we compare:
        - Values at time `t`
        - Values at time `t+1`

        **KS Statistic** shows:
        - **Low value (< 0.05)** ‚Üí Distributions are similar ‚Üí ‚úÖ stable
        - **High value (> 0.15)** ‚Üí Distributions differ ‚Üí ‚ùå potential shift

        This helps detect **distributional drift** or **structural change** in the series.
        """)
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            stat, _ = ks_2samp(ts_transformed[:-1], ts_transformed[1:])

        st.write(f"KS Statistic: {float(stat):.4f}")

        # Interpretation
        if stat < 0.05:
            st.success("‚úÖ The distributions are very similar. No significant shift detected.")
        elif 0.05 <= stat <= 0.15:
            st.info("‚ö†Ô∏è Slight distributional shift observed. Might be acceptable depending on use case.")
        else:
            st.warning("‚ùå Significant distributional difference detected between consecutive periods.")

    except Exception as e:
        st.error(f"KS Test failed: {e}")

    st.subheader("8Ô∏è‚É£ White Noise & Random Walk")
    with st.expander("‚ÑπÔ∏è About this section"):
        st.markdown("""
        This section checks whether the time series behaves like **white noise** (pure randomness).

        - **White noise** = no structure, no predictability
        - **Random walk** = previous value + noise

        The **first difference** of the series is plotted:
        - If it looks random ‚Üí series might be a **random walk**
        - If it shows pattern ‚Üí more structure exists, and it‚Äôs **modelable**

        Helps you judge whether the data is **predictable** or just noise.
        """)
    noise = ts_transformed.diff().dropna()
    plt.figure(figsize=(10, 3))
    plt.plot(noise)
    plt.title("White Noise Approximation (First Difference)")
    st.pyplot(plt)